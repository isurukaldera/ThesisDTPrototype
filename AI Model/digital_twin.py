# -*- coding: utf-8 -*-
"""Digital_Twin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RVqU3fYvzKavxbws0DPFiGfLAxHOSQST
"""

!pip install fastapi uvicorn pyngrok nest-asyncio requests scikit-learn pandas numpy holidays matplotlib seaborn
!pkill -f uvicorn
!pkill -f ngrok

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from pyngrok import ngrok
import nest_asyncio
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import uvicorn
import requests
import threading
import time
import json
import holidays
import random
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from collections import defaultdict
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64
from IPython.display import display, HTML, Image
import io

# Apply nest_asyncio for Colab compatibility
nest_asyncio.apply()

# ========== FIX 1: KILL PORT 8000 PROCESSES ==========
import subprocess
subprocess.run(["fuser", "-k", "8000/tcp"], capture_output=True)
time.sleep(2)

# Create FastAPI app
app = FastAPI(title="Enhanced Supermarket AI Forecasting Server")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

DB_PATH = "/content/digital_twin.db"

# Email Configuration for Critical Alerts
EMAIL_CONFIG = {
    "smtp_server": "smtp.gmail.com",
    "smtp_port": 587,
    "sender_email": "slgemba6@gmail.com",
    "sender_password": "cdhf qukc uaqx lnjd",
    "critical_alert_emails": [
        "isuru.kaldera6425@gmail.com",
        # Add staff member emails
    ]
}

class RecommendRequest(BaseModel):
    product_id: int
    period_days: int = 7
    historical_weeks: int = 8
    safety_buffer: float = 0.15
    include_promotions: bool = True

class EnhancedRecommendRequest(BaseModel):
    product_id: int
    period_days: int = 7
    historical_weeks: int = 8
    safety_buffer: float = 0.15
    include_promotions: bool = True

class PromotionRequest(BaseModel):
    product_id: int
    promotion_name: str
    discount_percentage: float
    start_date: str
    end_date: str
    expected_lift: float = 0.2

class SalesLogRequest(BaseModel):
    product_id: int
    quantity_sold: int
    sale_date: str = None
    was_promotion: bool = False

# ========== DATABASE SETUP ==========

def setup_database():
    """Create all necessary database tables"""
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    cur = conn.cursor()

    # Products table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS Products (
            ProductID INTEGER PRIMARY KEY,
            ProductName TEXT NOT NULL,
            Category TEXT NOT NULL,
            Price REAL NOT NULL,
            Size TEXT
        )
    """)

    # StoreStock table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS StoreStock (
            StockID INTEGER PRIMARY KEY AUTOINCREMENT,
            ProductID INTEGER NOT NULL,
            Quantity INTEGER NOT NULL,
            Location TEXT,
            FOREIGN KEY (ProductID) REFERENCES Products(ProductID)
        )
    """)

    # WarehouseStock table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS WarehouseStock (
            StockID INTEGER PRIMARY KEY AUTOINCREMENT,
            ProductID INTEGER NOT NULL,
            Quantity INTEGER NOT NULL,
            Location TEXT,
            FOREIGN KEY (ProductID) REFERENCES Products(ProductID)
        )
    """)

    # SalesHistory table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS SalesHistory (
            SalesID INTEGER PRIMARY KEY AUTOINCREMENT,
            ProductID INTEGER NOT NULL,
            SaleDate DATE NOT NULL,
            QuantitySold INTEGER NOT NULL,
            DayOfWeek INTEGER,
            WasPromotion BOOLEAN DEFAULT FALSE,
            FOREIGN KEY (ProductID) REFERENCES Products(ProductID)
        )
    """)

    # Promotions table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS Promotions (
            PromotionID INTEGER PRIMARY KEY AUTOINCREMENT,
            ProductID INTEGER NOT NULL,
            PromotionName TEXT NOT NULL,
            DiscountPercentage REAL NOT NULL,
            StartDate DATE NOT NULL,
            EndDate DATE NOT NULL,
            ExpectedLiftPercentage REAL DEFAULT 0.2,
            Status TEXT DEFAULT 'active',
            CreatedDate DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (ProductID) REFERENCES Products(ProductID)
        )
    """)

    # Seasonal patterns
    cur.execute("""
        CREATE TABLE IF NOT EXISTS SeasonalPatterns (
            PatternID INTEGER PRIMARY KEY AUTOINCREMENT,
            Category TEXT NOT NULL,
            Month INTEGER NOT NULL,
            SeasonalityFactor REAL NOT NULL,
            Description TEXT,
            UNIQUE(Category, Month)
        )
    """)

    # External factors
    cur.execute("""
        CREATE TABLE IF NOT EXISTS ExternalFactors (
            FactorID INTEGER PRIMARY KEY AUTOINCREMENT,
            FactorDate DATE NOT NULL,
            IsHoliday BOOLEAN DEFAULT FALSE,
            HolidayName TEXT,
            WeatherImpact REAL DEFAULT 1.0,
            EconomicIndex REAL DEFAULT 1.0,
            SpecialEvent TEXT
        )
    """)

    # Demand forecasts
    cur.execute("""
        CREATE TABLE IF NOT EXISTS DemandForecasts (
            ForecastID INTEGER PRIMARY KEY AUTOINCREMENT,
            ProductID INTEGER NOT NULL,
            ForecastDate DATE NOT NULL,
            PeriodDays INTEGER NOT NULL,
            PredictedDemand REAL NOT NULL,
            ConfidenceLevel REAL NOT NULL,
            ForecastType TEXT NOT NULL,
            Factors JSON,
            CreatedDate DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (ProductID) REFERENCES Products(ProductID)
        )
    """)

    # Prediction Accuracy table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS PredictionAccuracy (
            AccuracyID INTEGER PRIMARY KEY AUTOINCREMENT,
            ProductID INTEGER NOT NULL,
            PredictedDemand REAL NOT NULL,
            ActualDemand REAL,
            PeriodDays INTEGER NOT NULL,
            PredictionDate DATE NOT NULL,
            PeriodEndDate DATE NOT NULL,
            AccuracyScore REAL,
            IsEvaluated BOOLEAN DEFAULT FALSE,
            CreatedDate DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (ProductID) REFERENCES Products(ProductID)
        )
    """)

    # Critical alerts table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS CriticalAlerts (
            AlertID INTEGER PRIMARY KEY AUTOINCREMENT,
            ProductID INTEGER NOT NULL,
            ProductName TEXT NOT NULL,
            Category TEXT NOT NULL,
            ShelfStock INTEGER NOT NULL,
            WarehouseStock INTEGER NOT NULL,
            PredictedDemand REAL,
            AlertMessage TEXT NOT NULL,
            SentTo TEXT NOT NULL,
            SentDate DATETIME DEFAULT CURRENT_TIMESTAMP,
            IsSent BOOLEAN DEFAULT TRUE
        )
    """)

    conn.commit()
    conn.close()
    print("Database tables created successfully")

# Initialize database
setup_database()

# ========== IMPROVED DATABASE CONNECTION HANDLING ==========

def get_db_connection():
    """Get a database connection with proper timeout handling"""
    conn = sqlite3.connect(DB_PATH, check_same_thread=False, timeout=30.0)
    conn.execute("PRAGMA busy_timeout = 30000")  # 30 second timeout
    return conn

def execute_with_retry(query, params=(), max_retries=3):
    """Execute database query with retry logic for locked database"""
    for attempt in range(max_retries):
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute(query, params)
            conn.commit()
            conn.close()
            return True
        except sqlite3.OperationalError as e:
            if "locked" in str(e) and attempt < max_retries - 1:
                print(f"Database locked, retrying... (attempt {attempt + 1})")
                time.sleep(0.5 * (attempt + 1))
                continue
            else:
                print(f"Database error after {max_retries} attempts: {e}")
                return False
        except Exception as e:
            print(f"Database error: {e}")
            return False

# ========== VISUALIZATION COMPONENTS ==========

def create_linear_regression_diagram(product_id, display_plot=True):
    """Create linear regression diagram for sales forecasting"""
    try:
        conn = get_db_connection()

        # Get sales data for the product
        query = """
        SELECT SaleDate, QuantitySold
        FROM SalesHistory
        WHERE ProductID = ?
        ORDER BY SaleDate
        """
        df = pd.read_sql_query(query, conn, params=(product_id,))
        conn.close()

        if df.empty or len(df) < 7:
            return None

        # Prepare data for linear regression
        df['SaleDate'] = pd.to_datetime(df['SaleDate'])
        df = df.sort_values('SaleDate')
        df['Days'] = (df['SaleDate'] - df['SaleDate'].min()).dt.days

        # Create linear regression model
        X = df[['Days']].values
        y = df['QuantitySold'].values

        model = LinearRegression()
        model.fit(X, y)
        y_pred = model.predict(X)

        # Create the plot
        plt.figure(figsize=(12, 8))

        # Plot actual sales
        plt.scatter(df['Days'], y, alpha=0.7, label='Actual Sales', color='blue')

        # Plot regression line
        plt.plot(df['Days'], y_pred, color='red', linewidth=2, label='Linear Regression')

        # Add trend line equation
        equation = f'y = {model.coef_[0]:.2f}x + {model.intercept_:.2f}'
        r_squared = model.score(X, y)

        plt.title(f'Linear Regression Analysis\nProduct ID: {product_id}', fontsize=16, fontweight='bold')
        plt.xlabel('Days', fontsize=12)
        plt.ylabel('Quantity Sold', fontsize=12)
        plt.legend()
        plt.grid(True, alpha=0.3)

        # Add statistics
        stats_text = f'R² = {r_squared:.3f}\nEquation: {equation}\nTrend: {"Increasing" if model.coef_[0] > 0 else "Decreasing"}'
        plt.annotate(stats_text, xy=(0.02, 0.98), xycoords='axes fraction',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7),
                    verticalalignment='top', fontsize=10)

        # Convert plot to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()

        # Display plot if requested
        if display_plot:
            plt.show()
        else:
            plt.close()

        return image_base64

    except Exception as e:
        print(f"Error creating linear regression diagram: {e}")
        return None

def create_seasonal_sales_diagram(product_id, display_plot=True):
    """Create seasonal sales pattern diagram for specific products"""
    try:
        conn = get_db_connection()

        # Get product info and sales data
        product_query = "SELECT ProductName, Category FROM Products WHERE ProductID = ?"
        product_info = pd.read_sql_query(product_query, conn, params=(product_id,))

        if product_info.empty:
            conn.close()
            return None

        product_name = product_info.iloc[0]['ProductName']
        category = product_info.iloc[0]['Category']

        # Get monthly sales data
        sales_query = """
        SELECT
            strftime('%m', SaleDate) as Month,
            AVG(QuantitySold) as AvgSales,
            SUM(QuantitySold) as TotalSales
        FROM SalesHistory
        WHERE ProductID = ?
        GROUP BY strftime('%m', SaleDate)
        ORDER BY Month
        """
        monthly_sales = pd.read_sql_query(sales_query, conn, params=(product_id,))
        conn.close()

        if monthly_sales.empty:
            return None

        # Create the plot
        plt.figure(figsize=(14, 8))

        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

        # Create complete monthly data
        complete_data = []
        for i in range(1, 13):
            month_data = monthly_sales[monthly_sales['Month'] == f'{i:02d}']
            if not month_data.empty:
                complete_data.append({
                    'Month': months[i-1],
                    'AvgSales': month_data.iloc[0]['AvgSales'],
                    'TotalSales': month_data.iloc[0]['TotalSales']
                })
            else:
                complete_data.append({
                    'Month': months[i-1],
                    'AvgSales': 0,
                    'TotalSales': 0
                })

        complete_df = pd.DataFrame(complete_data)

        # Create subplots
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))

        # Plot 1: Average Sales by Month
        bars1 = ax1.bar(complete_df['Month'], complete_df['AvgSales'],
                       color='skyblue', alpha=0.7, edgecolor='navy')
        ax1.set_title(f'Average Monthly Sales - {product_name}\n({category})',
                     fontsize=14, fontweight='bold')
        ax1.set_ylabel('Average Daily Sales', fontsize=12)
        ax1.grid(True, alpha=0.3)

        # Add value labels on bars
        for bar in bars1:
            height = bar.get_height()
            if height > 0:
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}', ha='center', va='bottom', fontweight='bold')

        # Plot 2: Total Sales by Month
        bars2 = ax2.bar(complete_df['Month'], complete_df['TotalSales'],
                       color='lightcoral', alpha=0.7, edgecolor='darkred')
        ax2.set_title('Total Monthly Sales', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Total Sales', fontsize=12)
        ax2.set_xlabel('Month', fontsize=12)
        ax2.grid(True, alpha=0.3)

        # Add value labels on bars
        for bar in bars2:
            height = bar.get_height()
            if height > 0:
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}', ha='center', va='bottom', fontweight='bold')

        plt.tight_layout()

        # Convert plot to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()

        # Display plot if requested
        if display_plot:
            plt.show()
        else:
            plt.close()

        return image_base64

    except Exception as e:
        print(f"Error creating seasonal sales diagram: {e}")
        return None

def create_confidence_assessment_diagram(display_plot=True):
    """Create confidence assessment diagram for all products"""
    try:
        conn = get_db_connection()

        # Get confidence data for all products
        query = """
        SELECT
            p.ProductID,
            p.ProductName,
            p.Category,
            COUNT(sh.SalesID) as SalesCount,
            AVG(sh.QuantitySold) as AvgSales,
            CASE
                WHEN COUNT(sh.SalesID) >= 50 THEN 'High'
                WHEN COUNT(sh.SalesID) >= 20 THEN 'Medium'
                ELSE 'Low'
            END as DataQuality,
            CASE
                WHEN COUNT(sh.SalesID) >= 50 THEN 0.9
                WHEN COUNT(sh.SalesID) >= 20 THEN 0.7
                ELSE 0.4
            END as ConfidenceScore
        FROM Products p
        LEFT JOIN SalesHistory sh ON p.ProductID = sh.ProductID
        GROUP BY p.ProductID, p.ProductName, p.Category
        HAVING SalesCount > 0
        ORDER BY ConfidenceScore DESC, SalesCount DESC
        LIMIT 20
        """

        df = pd.read_sql_query(query, conn)
        conn.close()

        if df.empty:
            return None

        # Create the plot
        plt.figure(figsize=(16, 10))

        # Color coding based on confidence
        colors = []
        for score in df['ConfidenceScore']:
            if score >= 0.8:
                colors.append('green')
            elif score >= 0.6:
                colors.append('orange')
            else:
                colors.append('red')

        # Create horizontal bar chart
        y_pos = np.arange(len(df))

        plt.barh(y_pos, df['ConfidenceScore'], color=colors, alpha=0.7, edgecolor='black')
        plt.yticks(y_pos, df['ProductName'], fontsize=9)
        plt.xlabel('Confidence Score', fontsize=12)
        plt.title('AI Forecasting Confidence Assessment\n(Top 20 Products by Data Quality)',
                 fontsize=16, fontweight='bold')

        # Add value labels
        for i, v in enumerate(df['ConfidenceScore']):
            plt.text(v + 0.01, i, f'{v:.2f}', va='center', fontweight='bold')

        # Add data quality labels
        for i, (score, quality) in enumerate(zip(df['ConfidenceScore'], df['DataQuality'])):
            plt.text(0.02, i, quality, va='center', ha='left',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8),
                    fontweight='bold')

        plt.grid(True, alpha=0.3, axis='x')
        plt.xlim(0, 1.0)
        plt.tight_layout()

        # Add legend
        legend_elements = [
            plt.Rectangle((0,0),1,1, facecolor='green', alpha=0.7, label='High Confidence (≥0.8)'),
            plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.7, label='Medium Confidence (0.6-0.8)'),
            plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.7, label='Low Confidence (<0.6)')
        ]
        plt.legend(handles=legend_elements, loc='lower right')

        # Convert plot to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()

        # Display plot if requested
        if display_plot:
            plt.show()
        else:
            plt.close()

        return image_base64

    except Exception as e:
        print(f"Error creating confidence assessment diagram: {e}")
        return None

def create_promotion_effect_diagram(product_id, display_plot=True):
    """Create promotion effect analysis diagram"""
    try:
        conn = get_db_connection()

        # Get promotion and sales data
        promo_query = """
        SELECT
            p.PromotionName,
            p.DiscountPercentage,
            p.StartDate,
            p.EndDate,
            p.ExpectedLiftPercentage,
            AVG(CASE WHEN sh.WasPromotion = 1 THEN sh.QuantitySold ELSE NULL END) as AvgPromoSales,
            AVG(CASE WHEN sh.WasPromotion = 0 THEN sh.QuantitySold ELSE NULL END) as AvgRegularSales,
            COUNT(CASE WHEN sh.WasPromotion = 1 THEN 1 ELSE NULL END) as PromoDays,
            COUNT(CASE WHEN sh.WasPromotion = 0 THEN 1 ELSE NULL END) as RegularDays
        FROM Promotions p
        LEFT JOIN SalesHistory sh ON p.ProductID = sh.ProductID
            AND sh.SaleDate BETWEEN p.StartDate AND p.EndDate
        WHERE p.ProductID = ?
        GROUP BY p.PromotionID
        HAVING PromoDays > 0 AND RegularDays > 0
        """

        promo_data = pd.read_sql_query(promo_query, conn, params=(product_id,))

        # Get product info
        product_query = "SELECT ProductName FROM Products WHERE ProductID = ?"
        product_info = pd.read_sql_query(product_query, conn, params=(product_id,))
        conn.close()

        if promo_data.empty or product_info.empty:
            return None

        product_name = product_info.iloc[0]['ProductName']

        # Create the plot
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

        # Plot 1: Sales Comparison (Promo vs Regular)
        promotions = promo_data['PromotionName'].str[:20] + '...'  # Truncate long names
        regular_sales = promo_data['AvgRegularSales']
        promo_sales = promo_data['AvgPromoSales']

        x = np.arange(len(promotions))
        width = 0.35

        bars1 = ax1.bar(x - width/2, regular_sales, width, label='Regular Sales', alpha=0.7, color='blue')
        bars2 = ax1.bar(x + width/2, promo_sales, width, label='Promotion Sales', alpha=0.7, color='red')

        ax1.set_xlabel('Promotions')
        ax1.set_ylabel('Average Daily Sales')
        ax1.set_title('Sales Comparison: Promotion vs Regular\n' + product_name, fontweight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels(promotions, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Add value labels
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}', ha='center', va='bottom', fontsize=8)

        # Plot 2: Promotion Lift Analysis
        actual_lift = ((promo_data['AvgPromoSales'] - promo_data['AvgRegularSales']) / promo_data['AvgRegularSales']) * 100
        expected_lift = promo_data['ExpectedLiftPercentage'] * 100

        x = np.arange(len(promotions))
        bars1 = ax2.bar(x - width/2, expected_lift, width, label='Expected Lift', alpha=0.7, color='orange')
        bars2 = ax2.bar(x + width/2, actual_lift, width, label='Actual Lift', alpha=0.7, color='green')

        ax2.set_xlabel('Promotions')
        ax2.set_ylabel('Sales Lift (%)')
        ax2.set_title('Promotion Lift: Expected vs Actual', fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels(promotions, rotation=45, ha='right')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # Add value labels
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}%', ha='center', va='bottom', fontsize=8)

        # Plot 3: Discount vs Sales Lift
        discount = promo_data['DiscountPercentage']
        ax3.scatter(discount, actual_lift, s=100, alpha=0.7, color='purple')

        # Add trend line
        if len(discount) > 1:
            z = np.polyfit(discount, actual_lift, 1)
            p = np.poly1d(z)
            ax3.plot(discount, p(discount), "r--", alpha=0.8, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')

        ax3.set_xlabel('Discount Percentage (%)')
        ax3.set_ylabel('Actual Sales Lift (%)')
        ax3.set_title('Discount Impact on Sales Lift', fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # Add promotion labels
        for i, promo in enumerate(promotions):
            ax3.annotate(promo, (discount.iloc[i], actual_lift.iloc[i]),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)

        # Plot 4: Promotion Effectiveness Summary
        effectiveness = []
        labels = []
        colors = []

        for i, row in promo_data.iterrows():
            lift_ratio = actual_lift.iloc[i] / expected_lift.iloc[i] if expected_lift.iloc[i] > 0 else 0
            if lift_ratio >= 1.0:
                effectiveness.append(lift_ratio)
                labels.append(f"P{i+1}\n({row['DiscountPercentage']}%)")
                colors.append('green')
            elif lift_ratio >= 0.7:
                effectiveness.append(lift_ratio)
                labels.append(f"P{i+1}\n({row['DiscountPercentage']}%)")
                colors.append('orange')
            else:
                effectiveness.append(lift_ratio)
                labels.append(f"P{i+1}\n({row['DiscountPercentage']}%)")
                colors.append('red')

        if effectiveness:
            bars = ax4.bar(labels, effectiveness, color=colors, alpha=0.7, edgecolor='black')
            ax4.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Target (100%)')
            ax4.set_xlabel('Promotions (P1, P2, ... with Discount %)')
            ax4.set_ylabel('Effectiveness Ratio (Actual/Expected)')
            ax4.set_title('Promotion Effectiveness Summary', fontweight='bold')
            ax4.legend()
            ax4.grid(True, alpha=0.3)

            # Add value labels
            for bar in bars:
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.2f}', ha='center', va='bottom', fontweight='bold')

        plt.tight_layout()

        # Convert plot to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()

        # Display plot if requested
        if display_plot:
            plt.show()
        else:
            plt.close()

        return image_base64

    except Exception as e:
        print(f"Error creating promotion effect diagram: {e}")
        return None

# ========== VISUALIZATION ENDPOINTS ==========

@app.get("/visualization/linear-regression/{product_id}")
def get_linear_regression_diagram(product_id: int):
    """Get linear regression diagram for a product"""
    try:
        image_base64 = create_linear_regression_diagram(product_id, display_plot=False)
        if image_base64:
            return {
                "status": "success",
                "product_id": product_id,
                "image_type": "linear_regression",
                "image_data": f"data:image/png;base64,{image_base64}",
                "message": "Linear regression diagram generated successfully"
            }
        else:
            return {"status": "error", "message": "Could not generate linear regression diagram"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/visualization/seasonal-sales/{product_id}")
def get_seasonal_sales_diagram(product_id: int):
    """Get seasonal sales diagram for a product"""
    try:
        image_base64 = create_seasonal_sales_diagram(product_id, display_plot=False)
        if image_base64:
            return {
                "status": "success",
                "product_id": product_id,
                "image_type": "seasonal_sales",
                "image_data": f"data:image/png;base64,{image_base64}",
                "message": "Seasonal sales diagram generated successfully"
            }
        else:
            return {"status": "error", "message": "Could not generate seasonal sales diagram"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/visualization/confidence-assessment")
def get_confidence_assessment_diagram():
    """Get confidence assessment diagram for all products"""
    try:
        image_base64 = create_confidence_assessment_diagram(display_plot=False)
        if image_base64:
            return {
                "status": "success",
                "image_type": "confidence_assessment",
                "image_data": f"data:image/png;base64,{image_base64}",
                "message": "Confidence assessment diagram generated successfully"
            }
        else:
            return {"status": "error", "message": "Could not generate confidence assessment diagram"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/visualization/promotion-effect/{product_id}")
def get_promotion_effect_diagram(product_id: int):
    """Get promotion effect diagram for a product"""
    try:
        image_base64 = create_promotion_effect_diagram(product_id, display_plot=False)
        if image_base64:
            return {
                "status": "success",
                "product_id": product_id,
                "image_type": "promotion_effect",
                "image_data": f"data:image/png;base64,{image_base64}",
                "message": "Promotion effect diagram generated successfully"
            }
        else:
            return {"status": "error", "message": "Could not generate promotion effect diagram"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/visualization/all-diagrams/{product_id}")
def get_all_diagrams(product_id: int):
    """Get all visualization diagrams for a product"""
    try:
        diagrams = {}

        # Linear Regression
        lr_image = create_linear_regression_diagram(product_id, display_plot=False)
        if lr_image:
            diagrams["linear_regression"] = f"data:image/png;base64,{lr_image}"

        # Seasonal Sales
        ss_image = create_seasonal_sales_diagram(product_id, display_plot=False)
        if ss_image:
            diagrams["seasonal_sales"] = f"data:image/png;base64,{ss_image}"

        # Confidence Assessment
        ca_image = create_confidence_assessment_diagram(display_plot=False)
        if ca_image:
            diagrams["confidence_assessment"] = f"data:image/png;base64,{ca_image}"

        # Promotion Effect
        pe_image = create_promotion_effect_diagram(product_id, display_plot=False)
        if pe_image:
            diagrams["promotion_effect"] = f"data:image/png;base64,{pe_image}"

        return {
            "status": "success",
            "product_id": product_id,
            "diagrams": diagrams,
            "message": f"Generated {len(diagrams)} diagrams successfully"
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}

# ========== CRITICAL STOCK ALERT SYSTEM ==========

class CriticalStockAlertSystem:
    def __init__(self, config):
        self.config = config

    def send_critical_stock_alert(self, product_id, product_name, category, shelf_stock, warehouse_stock, predicted_demand, transfer_units, order_units):
        """Send critical stock alert to all recipients in the exact format specified"""
        try:
            # Create the exact email content as specified
            subject = "SUPERMARKET INVENTORY ALERT"

            # Format the message exactly as specified
            message = f"""SUPERMARKET INVENTORY ALERT

Severity: CRITICAL

Product Details:
Product ID: {product_id}

Product Name: {product_name}

Category: {category}

Current Shelf Stock: {shelf_stock} units

Current Warehouse Stock: {warehouse_stock} units

Predicted Daily Demand: N/A units/day

Alert Details:
Type: Critical Stock

Message: Product has only {shelf_stock} units left on shelf

Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

RECOMMENDED ACTIONS:
IMMEDIATE TRANSFER from warehouse: {transfer_units} units
URGENT ORDER from supplier: {order_units} units
Check sales data for unexpected demand spikes
Verify physical stock count
Monitor stock levels hourly"""

            # Create plain text email (no HTML)
            msg = MIMEMultipart()
            msg['From'] = self.config["sender_email"]
            msg['To'] = ", ".join(self.config["critical_alert_emails"])
            msg['Subject'] = subject

            # Attach plain text message
            msg.attach(MIMEText(message, 'plain'))

            print(f"Attempting to send critical alert to {len(self.config['critical_alert_emails'])} recipients...")
            server = smtplib.SMTP(self.config["smtp_server"], self.config["smtp_port"])
            server.starttls()
            print("Logging into SMTP server...")
            server.login(self.config["sender_email"], self.config["sender_password"])
            print("Login successful, sending critical alert...")
            text = msg.as_string()
            server.sendmail(self.config["sender_email"], self.config["critical_alert_emails"], text)
            server.quit()

            # Log the critical alert
            self.log_critical_alert(product_id, product_name, category, shelf_stock, warehouse_stock, predicted_demand, message)

            print(f"SUCCESS: Critical alert sent for {product_name} to {len(self.config['critical_alert_emails'])} recipients")
            return True

        except Exception as e:
            print(f"FAILED to send critical alert: {e}")
            return False

    def log_critical_alert(self, product_id, product_name, category, shelf_stock, warehouse_stock, predicted_demand, message):
        """Log critical alert to database"""
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("""
                INSERT INTO CriticalAlerts (ProductID, ProductName, Category, ShelfStock, WarehouseStock, PredictedDemand, AlertMessage, SentTo)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (product_id, product_name, category, shelf_stock, warehouse_stock, predicted_demand, message, ", ".join(self.config["critical_alert_emails"])))
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"Error logging critical alert: {e}")

    def get_critical_alert_history(self, days: int = 7):
        """Get history of critical alerts"""
        try:
            conn = get_db_connection()
            query = """
            SELECT AlertID, ProductID, ProductName, Category, ShelfStock, WarehouseStock, SentDate
            FROM CriticalAlerts
            WHERE SentDate >= datetime('now', '-' || ? || ' days')
            ORDER BY SentDate DESC
            """
            df = pd.read_sql_query(query, conn, params=(days,))
            conn.close()
            return df.to_dict('records')
        except Exception as e:
            print(f"Error getting critical alert history: {e}")
            return []

# Initialize critical stock alert system
critical_alert_system = CriticalStockAlertSystem(EMAIL_CONFIG)

# ========== REALISTIC FAKE DATA GENERATION ==========

def generate_realistic_brands_and_products():
    """Generate realistic products with actual brands for all categories"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()

        # Clear existing products
        cur.execute("DELETE FROM Products WHERE ProductID > 0")

        # Realistic products with actual brands organized by category
        realistic_products = {
            'Dairy': [
                # Milk & Cream
                ("Organic Valley Whole Milk", "Dairy", 4.99, "gallon"),
                ("Horizon Organic 2% Milk", "Dairy", 5.49, "gallon"),
                ("Silk Almond Milk Unsweetened", "Dairy", 3.99, "half gallon"),
                ("Fairlife Ultra-Filtered Milk", "Dairy", 4.79, "52oz"),
                ("Lactaid Whole Milk", "Dairy", 5.29, "gallon"),

                # Cheese
                ("Kraft Sharp Cheddar", "Dairy", 3.99, "16oz block"),
                ("Sargento Mozzarella", "Dairy", 4.29, "16oz shredded"),
                ("Philadelphia Cream Cheese", "Dairy", 2.99, "8oz"),
                ("Tillamook Medium Cheddar", "Dairy", 6.99, "24oz block"),
                ("Brie Cheese Wheel", "Dairy", 8.99, "8oz"),

                # Yogurt & Butter
                ("Chobani Greek Yogurt", "Dairy", 1.29, "5.3oz"),
                ("Yoplait Original Yogurt", "Dairy", 0.89, "6oz"),
                ("Land O Lakes Butter", "Dairy", 4.99, "16oz"),
                ("Kerrygold Pure Irish Butter", "Dairy", 5.49, "8oz"),
                ("Activia Probiotic Yogurt", "Dairy", 4.99, "4-pack")
            ],

            'Beverages': [
                # Sodas
                ("Coca-Cola Classic", "Beverages", 2.99, "12-pack 12oz"),
                ("Pepsi Cola", "Beverages", 2.79, "12-pack 12oz"),
                ("Sprite Lemon-Lime", "Beverages", 2.99, "12-pack 12oz"),
                ("Mountain Dew", "Beverages", 2.79, "12-pack 12oz"),
                ("Dr Pepper", "Beverages", 2.99, "12-pack 12oz"),

                # Juices
                ("Tropicana Orange Juice", "Beverages", 4.99, "59oz"),
                ("Simply Orange Juice", "Beverages", 5.49, "52oz"),
                ("Minute Maid Apple Juice", "Beverages", 3.49, "64oz"),
                ("Ocean Spray Cranberry Juice", "Beverages", 4.29, "64oz"),
                ("V8 Vegetable Juice", "Beverages", 3.99, "46oz"),

                # Water & Energy Drinks
                ("Aquafina Purified Water", "Beverages", 4.99, "24-pack"),
                ("Dasani Purified Water", "Beverages", 5.49, "24-pack"),
                ("Red Bull Energy Drink", "Beverages", 15.99, "8-pack"),
                ("Monster Energy Drink", "Beverages", 14.99, "12-pack"),
                ("Gatorade Thirst Quencher", "Beverages", 7.99, "8-pack")
            ],

            'Frozen': [
                # Frozen Meals
                ("Stouffer's Lasagna", "Frozen", 8.99, "40oz"),
                ("Lean Cuisine Chicken Alfredo", "Frozen", 3.99, "9.5oz"),
                ("Amy's Kitchen Mac & Cheese", "Frozen", 4.49, "9.5oz"),
                ("Hot Pockets Ham & Cheese", "Frozen", 6.99, "8-pack"),
                ("Digiorno Rising Crust Pizza", "Frozen", 6.99, "27.5oz"),

                # Frozen Vegetables
                ("Birds Eye Mixed Vegetables", "Frozen", 2.99, "16oz"),
                ("Green Giant Broccoli Florets", "Frozen", 2.49, "12oz"),
                ("C&W Corn Kernels", "Frozen", 1.99, "16oz"),
                ("Birds Eye Steamfresh Peas", "Frozen", 2.29, "10oz"),

                # Frozen Desserts
                ("Ben & Jerry's Ice Cream", "Frozen", 5.99, "pint"),
                ("Häagen-Dazs Vanilla", "Frozen", 5.49, "pint"),
                ("Breyers Natural Vanilla", "Frozen", 4.99, "48oz"),
                ("Edy's Slow Churned Ice Cream", "Frozen", 4.49, "48oz")
            ],

            'Meat': [
                # Beef
                ("USDA Choice Ribeye Steak", "Meat", 15.99, "per lb"),
                ("Ground Beef 80/20", "Meat", 5.99, "per lb"),
                ("Beef Chuck Roast", "Meat", 7.99, "per lb"),
                ("USDA Prime NY Strip", "Meat", 18.99, "per lb"),

                # Pork
                ("Pork Loin Chops", "Meat", 6.99, "per lb"),
                ("Smithfield Bacon", "Meat", 7.99, "16oz"),
                ("Pork Tenderloin", "Meat", 8.99, "per lb"),
                ("Italian Sausage Links", "Meat", 5.99, "19oz"),

                # Poultry
                ("Perdue Chicken Breast", "Meat", 8.99, "per lb"),
                ("Tyson Chicken Thighs", "Meat", 5.99, "per lb"),
                ("Butterball Turkey Breast", "Meat", 12.99, "3lbs"),
                ("Foster Farms Whole Chicken", "Meat", 1.99, "per lb")
            ],

            'Snacks': [
                # Chips
                ("Lay's Classic Potato Chips", "Snacks", 4.99, "10oz"),
                ("Doritos Nacho Cheese", "Snacks", 4.79, "11oz"),
                ("Cheetos Crunchy", "Snacks", 4.49, "8.5oz"),
                ("Pringles Original", "Snacks", 2.99, "5.7oz"),
                ("Ruffles Original", "Snacks", 4.99, "9.5oz"),

                # Cookies
                ("Oreo Chocolate Sandwich", "Snacks", 4.29, "15.35oz"),
                ("Chips Ahoy! Chocolate Chip", "Snacks", 3.99, "13oz"),
                ("Pepperidge Farm Milano", "Snacks", 4.79, "7oz"),
                ("Nutter Butter Peanut Butter", "Snacks", 3.49, "16oz"),

                # Crackers & Nuts
                ("Ritz Crackers", "Snacks", 4.29, "13.7oz"),
                ("Wheat Thins Original", "Snacks", 3.99, "9oz"),
                ("Planters Mixed Nuts", "Snacks", 8.99, "16oz"),
                ("Blue Diamond Almonds", "Snacks", 7.49, "16oz")
            ],

            'Bakery': [
                # Bread
                ("Wonder Classic White Bread", "Bakery", 2.99, "20oz"),
                ("Nature's Own Whole Wheat", "Bakery", 3.49, "20oz"),
                ("Sara Lee Artesano Bread", "Bakery", 4.29, "20oz"),
                ("Pepperidge Farm Farmhouse", "Bakery", 4.99, "24oz"),

                # Baked Goods
                ("Entenmann's Little Bites", "Bakery", 4.49, "10.5oz"),
                ("Hostess Donettes", "Bakery", 3.99, "10.5oz"),
                ("Thomas English Muffins", "Bakery", 3.79, "6-pack"),
                ("Pillsbury Crescent Rolls", "Bakery", 3.49, "8oz")
            ],

            'Alcohol': [
                # Beer
                ("Budweiser Lager", "Alcohol", 14.99, "12-pack"),
                ("Coors Light", "Alcohol", 13.99, "12-pack"),
                ("Heineken Lager", "Alcohol", 16.99, "12-pack"),
                ("Corona Extra", "Alcohol", 15.99, "12-pack"),

                # Wine
                ("Robert Mondavi Cabernet", "Alcohol", 12.99, "750ml"),
                ("Barefoot Pinot Grigio", "Alcohol", 8.99, "750ml"),
                ("Yellow Tail Shiraz", "Alcohol", 9.99, "750ml"),
                ("Sutter Home Merlot", "Alcohol", 7.99, "750ml")
            ],

            'Toiletries': [
                # Personal Care
                ("Dove Beauty Bar", "Toiletries", 8.99, "8-pack"),
                ("Crest 3D White Toothpaste", "Toiletries", 4.99, "6oz"),
                ("Colgate Total Toothpaste", "Toiletries", 3.99, "6oz"),
                ("Gillette Fusion Razor", "Toiletries", 12.99, "4 blades"),
                ("Old Spice Deodorant", "Toiletries", 5.99, "2.7oz")
            ],

            'Cleaning': [
                # Household Cleaners
                ("Tide Liquid Laundry Detergent", "Cleaning", 12.99, "92oz"),
                ("Clorox Disinfecting Wipes", "Cleaning", 5.99, "75ct"),
                ("Dawn Dish Soap", "Cleaning", 3.99, "24oz"),
                ("Lysol All Purpose Cleaner", "Cleaning", 4.49, "32oz"),
                ("Swiffer Wet Jet Solution", "Cleaning", 8.99, "32oz")
            ],

            'Pet': [
                # Pet Food
                ("Purina Dog Chow", "Pet", 24.99, "36lb"),
                ("Iams Proactive Health Cat", "Pet", 19.99, "16lb"),
                ("Pedigree Canned Dog Food", "Pet", 1.29, "13.2oz"),
                ("Friskies Canned Cat Food", "Pet", 0.89, "5.5oz")
            ],

            'Baby': [
                # Baby Products
                ("Pampers Swaddlers", "Baby", 39.99, "size 3, 148ct"),
                ("Huggies Little Snugglers", "Baby", 37.99, "size 3, 140ct"),
                ("Enfamil Infant Formula", "Baby", 34.99, "23.4oz"),
                ("Similac Pro-Advance", "Baby", 36.99, "23.2oz")
            ]
        }

        # Insert all products
        product_id = 1
        for category, products in realistic_products.items():
            for product_name, category, price, size in products:
                cur.execute("""
                    INSERT INTO Products (ProductID, ProductName, Category, Price, Size)
                    VALUES (?, ?, ?, ?, ?)
                """, (product_id, product_name, category, price, size))
                product_id += 1

        conn.commit()
        conn.close()
        print(f"Generated {product_id-1} realistic products with actual brands")
    except Exception as e:
        print(f"Error generating products: {e}")

def generate_one_year_sales_data():
    """Generate comprehensive sales data for one full year with realistic patterns"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()

        # Clear existing sales data
        cur.execute("DELETE FROM SalesHistory WHERE SalesID > 0")

        # Get all products
        cur.execute("SELECT ProductID, Category FROM Products")
        products = cur.fetchall()

        # Generate 365 days of sales data (one full year)
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=365)

        # Seasonal multipliers by category and month
        seasonal_patterns = {
            'Dairy': [1.1, 1.0, 1.0, 1.0, 1.0, 1.1, 1.2, 1.3, 1.1, 1.0, 1.2, 1.3],
            'Beverages': [0.8, 0.8, 0.9, 1.0, 1.3, 1.6, 1.8, 1.7, 1.2, 1.0, 0.9, 1.1],
            'Frozen': [1.2, 1.1, 1.0, 0.8, 0.6, 0.5, 0.5, 0.6, 0.9, 1.1, 1.3, 1.4],
            'Meat': [1.1, 1.0, 0.9, 0.9, 1.2, 1.4, 1.5, 1.4, 1.1, 1.0, 1.3, 1.6],
            'Snacks': [1.3, 1.2, 1.1, 1.0, 0.8, 0.7, 0.8, 0.9, 1.0, 1.1, 1.4, 1.6],
            'Bakery': [1.2, 1.1, 1.0, 1.0, 1.0, 1.1, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],
            'Alcohol': [1.3, 1.1, 1.2, 1.0, 1.1, 1.3, 1.4, 1.3, 1.1, 1.0, 1.4, 1.8],
            'Toiletries': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.1, 1.2, 1.0, 1.0, 1.0],
            'Cleaning': [1.2, 1.1, 1.3, 1.1, 1.0, 1.0, 1.0, 1.0, 1.2, 1.1, 1.0, 1.1],
            'Pet': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
            'Baby': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
        }

        # Base daily sales by category (average units per day)
        base_daily_sales = {
            'Dairy': 15, 'Beverages': 25, 'Frozen': 8, 'Meat': 6, 'Snacks': 20,
            'Bakery': 12, 'Alcohol': 10, 'Toiletries': 5, 'Cleaning': 4, 'Pet': 3, 'Baby': 2
        }

        current_date = start_date
        total_records = 0
        batch_size = 1000
        batch_records = []

        while current_date <= end_date:
            month = current_date.month - 1
            day_of_week = current_date.weekday()
            is_weekend = day_of_week >= 5

            # Special events and holidays
            holiday_boost = 1.0

            # Major holidays with sales boosts
            if current_date.month == 12 and current_date.day in [24, 25, 31]:
                holiday_boost = 2.5
            elif current_date.month == 11 and current_date.day in [23, 24, 25]:
                holiday_boost = 2.0
            elif current_date.month == 7 and current_date.day == 4:
                holiday_boost = 1.8
            elif current_date.month == 12 and 15 <= current_date.day <= 23:
                holiday_boost = 1.6
            elif current_date.month == 5 and current_date.weekday() == 0 and 25 <= current_date.day <= 31:
                holiday_boost = 1.5
            elif current_date.month == 9 and current_date.weekday() == 0 and 1 <= current_date.day <= 7:
                holiday_boost = 1.4

            for product_id, category in products:
                base_sales = base_daily_sales.get(category, 6)
                seasonal_factor = seasonal_patterns.get(category, [1.0]*12)[month]
                weekend_factor = 1.3 if is_weekend else 1.0

                daily_target = base_sales * seasonal_factor * weekend_factor * holiday_boost
                random_variation = np.random.uniform(0.2, 1.8)
                final_sales = int(daily_target * random_variation)
                final_sales = max(1, final_sales)

                if np.random.random() > 0.15:
                    batch_records.append((product_id, current_date, final_sales, day_of_week, False))
                    total_records += 1

                    # Insert in batches to avoid memory issues
                    if len(batch_records) >= batch_size:
                        cur.executemany("""
                            INSERT INTO SalesHistory (ProductID, SaleDate, QuantitySold, DayOfWeek, WasPromotion)
                            VALUES (?, ?, ?, ?, ?)
                        """, batch_records)
                        batch_records = []

            current_date += timedelta(days=1)

        # Insert remaining records
        if batch_records:
            cur.executemany("""
                INSERT INTO SalesHistory (ProductID, SaleDate, QuantitySold, DayOfWeek, WasPromotion)
                VALUES (?, ?, ?, ?, ?)
            """, batch_records)

        conn.commit()
        conn.close()
        print(f"Generated {total_records} sales records for one full year ({start_date} to {end_date})")
    except Exception as e:
        print(f"Error generating sales data: {e}")

def generate_realistic_promotions():
    """Generate realistic promotional campaigns for one year"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()

        # Clear existing promotions
        cur.execute("DELETE FROM Promotions WHERE PromotionID > 0")

        # Get all products
        cur.execute("SELECT ProductID, ProductName, Category FROM Products")
        products = cur.fetchall()

        # Major promotional campaigns throughout the year
        promotional_campaigns = [
            {
                'name': "Winter Clearance Sale",
                'months': [1],
                'discount_range': (15, 40),
                'lift_range': (0.2, 0.4),
                'categories': ['Frozen', 'Snacks', 'Beverages'],
                'duration_range': (7, 14)
            },
            {
                'name': "Valentine's Day Special",
                'months': [2],
                'discount_range': (10, 25),
                'lift_range': (0.15, 0.3),
                'categories': ['Alcohol', 'Snacks'],
                'duration_range': (10, 14)
            },
            {
                'name': "Spring Refresh Sale",
                'months': [3, 4],
                'discount_range': (10, 30),
                'lift_range': (0.15, 0.35),
                'categories': ['Cleaning', 'Toiletries', 'Beverages'],
                'duration_range': (14, 21)
            },
            {
                'name': "Summer BBQ Season",
                'months': [5, 6, 7, 8],
                'discount_range': (10, 25),
                'lift_range': (0.2, 0.4),
                'categories': ['Meat', 'Beverages', 'Snacks', 'Frozen'],
                'duration_range': (7, 21)
            },
            {
                'name': "Back to School Sale",
                'months': [8, 9],
                'discount_range': (15, 30),
                'lift_range': (0.2, 0.35),
                'categories': ['Toiletries', 'Snacks', 'Cleaning'],
                'duration_range': (14, 28)
            },
            {
                'name': "Holiday Season Promo",
                'months': [11, 12],
                'discount_range': (10, 40),
                'lift_range': (0.25, 0.5),
                'categories': ['Alcohol', 'Snacks', 'Frozen', 'Bakery', 'Dairy'],
                'duration_range': (7, 21)
            }
        ]

        promotion_id = 1
        current_year = datetime.now().year
        promotions_to_update = []

        for campaign in promotional_campaigns:
            for month in campaign['months']:
                eligible_products = [p for p in products if p[2] in campaign['categories']]
                num_promotions = int(len(eligible_products) * np.random.uniform(0.3, 0.6))
                promoted_products = np.random.choice(
                    len(eligible_products),
                    size=min(num_promotions, len(eligible_products)),
                    replace=False
                )

                for idx in promoted_products:
                    product_id, product_name, category = eligible_products[idx]

                    start_day = np.random.randint(1, 28)
                    start_date = datetime(current_year, month, start_day).date()
                    duration = np.random.randint(campaign['duration_range'][0], campaign['duration_range'][1] + 1)
                    end_date = start_date + timedelta(days=duration)

                    if end_date > datetime.now().date():
                        continue

                    discount = np.random.randint(campaign['discount_range'][0], campaign['discount_range'][1] + 1)
                    lift = np.random.uniform(campaign['lift_range'][0], campaign['lift_range'][1])

                    promotion_name = f"{campaign['name']} - {product_name}"

                    cur.execute("""
                        INSERT INTO Promotions
                        (ProductID, PromotionName, DiscountPercentage, StartDate, EndDate, ExpectedLiftPercentage, Status)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (product_id, promotion_name, discount, start_date, end_date, lift, 'expired'))

                    promotion_id += 1
                    promotions_to_update.append((product_id, start_date, end_date, lift))

        # Create some currently active promotions
        active_campaigns = [
            {
                'name': "Current Weekly Specials",
                'discount_range': (10, 25),
                'lift_range': (0.15, 0.3),
                'categories': ['Dairy', 'Beverages', 'Snacks', 'Bakery'],
                'duration_range': (7, 14)
            }
        ]

        for campaign in active_campaigns:
            eligible_products = [p for p in products if p[2] in campaign['categories']]
            num_active = min(8, len(eligible_products))
            active_products = np.random.choice(len(eligible_products), size=num_active, replace=False)

            for idx in active_products:
                product_id, product_name, category = eligible_products[idx]

                start_date = datetime.now().date() - timedelta(days=np.random.randint(0, 3))
                duration = np.random.randint(campaign['duration_range'][0], campaign['duration_range'][1] + 1)
                end_date = start_date + timedelta(days=duration)

                discount = np.random.randint(campaign['discount_range'][0], campaign['discount_range'][1] + 1)
                lift = np.random.uniform(campaign['lift_range'][0], campaign['lift_range'][1])

                promotion_name = f"{campaign['name']} - {product_name}"

                cur.execute("""
                    INSERT INTO Promotions
                    (ProductID, PromotionName, DiscountPercentage, StartDate, EndDate, ExpectedLiftPercentage, Status)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (product_id, promotion_name, discount, start_date, end_date, lift, 'active'))

                promotion_id += 1
                promotions_to_update.append((product_id, start_date, end_date, lift))

        conn.commit()
        conn.close()

        # Update promotional sales after closing the main connection
        print("Updating promotional sales data...")
        for product_id, start_date, end_date, lift_factor in promotions_to_update:
            update_promotional_sales(product_id, start_date, end_date, lift_factor)

        print(f"Generated {promotion_id-1} realistic promotional campaigns")
    except Exception as e:
        print(f"Error generating promotions: {e}")

def update_promotional_sales(product_id, start_date, end_date, lift_factor):
    """Update sales history to reflect promotional lifts"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()

        # Get sales during promotion period
        cur.execute("""
            SELECT SalesID, QuantitySold FROM SalesHistory
            WHERE ProductID = ? AND SaleDate BETWEEN ? AND ?
        """, (product_id, start_date, end_date))

        promotional_sales = cur.fetchall()

        # Update sales in batches
        batch_updates = []
        for sales_id, original_quantity in promotional_sales:
            boosted_quantity = int(original_quantity * (1 + lift_factor))
            final_quantity = int(boosted_quantity * np.random.uniform(0.8, 1.2))
            batch_updates.append((final_quantity, sales_id))

        # Execute batch update
        if batch_updates:
            cur.executemany("""
                UPDATE SalesHistory
                SET QuantitySold = ?, WasPromotion = TRUE
                WHERE SalesID = ?
            """, batch_updates)

        conn.commit()
        conn.close()
    except Exception as e:
        print(f"Error updating promotional sales for product {product_id}: {e}")

def initialize_realistic_stock_levels():
    """Initialize realistic stock levels for all products"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()

        # Clear existing stock
        cur.execute("DELETE FROM StoreStock WHERE StockID > 0")
        cur.execute("DELETE FROM WarehouseStock WHERE StockID > 0")

        # Get all products
        cur.execute("SELECT ProductID, Category FROM Products")
        products = cur.fetchall()

        # Base stock levels by category (shelf, warehouse)
        base_stock_levels = {
            'Dairy': (40, 200),
            'Beverages': (60, 300),
            'Frozen': (25, 120),
            'Meat': (20, 80),
            'Snacks': (50, 250),
            'Bakery': (30, 150),
            'Alcohol': (25, 100),
            'Toiletries': (15, 75),
            'Cleaning': (12, 60),
            'Pet': (10, 50),
            'Baby': (8, 40)
        }

        stock_updates = []
        for product_id, category in products:
            shelf_base, warehouse_base = base_stock_levels.get(category, (20, 100))
            shelf_stock = max(5, int(shelf_base * np.random.uniform(0.7, 1.3)))
            warehouse_stock = max(20, int(warehouse_base * np.random.uniform(0.8, 1.5)))

            stock_updates.append((product_id, shelf_stock, 'Aisle 1'))
            stock_updates.append((product_id, warehouse_stock, 'Main Warehouse'))

        # Batch insert store stock
        cur.executemany("""
            INSERT INTO StoreStock (ProductID, Quantity, Location)
            VALUES (?, ?, ?)
        """, [(pid, qty, loc) for pid, qty, loc in stock_updates if 'Aisle' in loc])

        # Batch insert warehouse stock
        cur.executemany("""
            INSERT INTO WarehouseStock (ProductID, Quantity, Location)
            VALUES (?, ?, ?)
        """, [(pid, qty, loc) for pid, qty, loc in stock_updates if 'Warehouse' in loc])

        conn.commit()
        conn.close()
        print("Initialized realistic stock levels for all products")
    except Exception as e:
        print(f"Error initializing stock levels: {e}")

def generate_enhanced_sample_data():
    """Generate enhanced sample data using realistic brands and patterns"""
    print("Generating enhanced realistic sample data...")
    generate_realistic_brands_and_products()
    generate_one_year_sales_data()
    generate_realistic_promotions()
    initialize_realistic_stock_levels()
    print("Enhanced realistic sample data generation complete!")

# ========== ENHANCED DATA GENERATION ENDPOINTS ==========

@app.post("/generate-realistic-products")
def generate_realistic_products_endpoint():
    """Generate realistic products with actual brands"""
    try:
        generate_realistic_brands_and_products()
        return {"status": "success", "message": "Realistic products with brands generated"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/generate-one-year-data")
def generate_one_year_data_endpoint():
    """Generate one year of comprehensive sales data"""
    try:
        generate_one_year_sales_data()
        return {"status": "success", "message": "One year of sales data generated"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/generate-realistic-promotions")
def generate_realistic_promotions_endpoint():
    """Generate realistic promotional campaigns"""
    try:
        generate_realistic_promotions()
        return {"status": "success", "message": "Realistic promotional campaigns generated"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/initialize-realistic-stock")
def initialize_realistic_stock_endpoint():
    """Initialize realistic stock levels"""
    try:
        initialize_realistic_stock_levels()
        return {"status": "success", "message": "Realistic stock levels initialized"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/generate-complete-dataset")
def generate_complete_dataset_endpoint():
    """Generate complete realistic dataset (products, sales, promotions, stock)"""
    try:
        print("Starting complete dataset generation...")
        generate_realistic_brands_and_products()
        print("✓ Products generated")
        generate_one_year_sales_data()
        print("✓ Sales data generated")
        generate_realistic_promotions()
        print("✓ Promotions generated")
        initialize_realistic_stock_levels()
        print("✓ Stock levels initialized")

        return {
            "status": "success",
            "message": "Complete realistic dataset generated",
            "components": [
                "Realistic products with actual brands",
                "One year of comprehensive sales data",
                "Seasonal promotional campaigns",
                "Realistic stock levels"
            ]
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

# ========== DATASET ANALYSIS ENDPOINTS ==========

@app.get("/dataset-stats")
def get_dataset_stats():
    """Get statistics about the generated dataset"""
    try:
        conn = get_db_connection()

        # Product stats
        product_query = "SELECT COUNT(*) as total_products, COUNT(DISTINCT Category) as categories FROM Products"
        product_stats = pd.read_sql_query(product_query, conn).iloc[0]

        # Sales stats
        sales_query = """
        SELECT
            COUNT(*) as total_sales_records,
            SUM(QuantitySold) as total_units_sold,
            MIN(SaleDate) as first_sale_date,
            MAX(SaleDate) as last_sale_date,
            AVG(QuantitySold) as avg_daily_sales
        FROM SalesHistory
        """
        sales_stats = pd.read_sql_query(sales_query, conn).iloc[0]

        # Promotion stats
        promo_query = """
        SELECT
            COUNT(*) as total_promotions,
            COUNT(CASE WHEN Status = 'active' THEN 1 END) as active_promotions,
            AVG(DiscountPercentage) as avg_discount,
            AVG(ExpectedLiftPercentage) as avg_expected_lift
        FROM Promotions
        """
        promo_stats = pd.read_sql_query(promo_query, conn).iloc[0]

        # Stock stats
        stock_query = """
        SELECT
            SUM(s.Quantity) as total_shelf_stock,
            SUM(w.Quantity) as total_warehouse_stock
        FROM StoreStock s, WarehouseStock w
        WHERE s.ProductID = w.ProductID
        """
        stock_stats = pd.read_sql_query(stock_query, conn).iloc[0]

        conn.close()

        return {
            "status": "success",
            "dataset_statistics": {
                "products": {
                    "total_products": int(product_stats['total_products']),
                    "categories": int(product_stats['categories'])
                },
                "sales": {
                    "total_sales_records": int(sales_stats['total_sales_records']),
                    "total_units_sold": int(sales_stats['total_units_sold']),
                    "date_range": f"{sales_stats['first_sale_date']} to {sales_stats['last_sale_date']}",
                    "average_daily_sales": round(sales_stats['avg_daily_sales'], 2)
                },
                "promotions": {
                    "total_promotions": int(promo_stats['total_promotions']),
                    "active_promotions": int(promo_stats['active_promotions']),
                    "average_discount": round(promo_stats['avg_discount'], 2),
                    "average_expected_lift": round(promo_stats['avg_expected_lift'] * 100, 2)
                },
                "inventory": {
                    "total_shelf_stock": int(stock_stats['total_shelf_stock']),
                    "total_warehouse_stock": int(stock_stats['total_warehouse_stock'])
                }
            }
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

# ========== ENHANCED FORECASTING ENGINE ==========

class EnhancedForecastingEngine:
    def __init__(self):
        self.initialize_seasonal_patterns()
        self.initialize_holidays()

    def initialize_seasonal_patterns(self):
        """Initialize default seasonal patterns for different categories"""
        try:
            seasonal_data = {
                'Dairy': [
                    (1, 0.9, 'Winter low'), (2, 0.85, 'Winter low'), (3, 0.9, 'Early spring'),
                    (4, 1.0, 'Spring'), (5, 1.1, 'Late spring'), (6, 1.2, 'Summer high'),
                    (7, 1.25, 'Peak summer'), (8, 1.2, 'Summer'), (9, 1.1, 'Early fall'),
                    (10, 1.0, 'Fall'), (11, 0.95, 'Late fall'), (12, 1.1, 'Holiday season')
                ],
                'Beverages': [
                    (1, 0.8, 'Winter low'), (2, 0.8, 'Winter low'), (3, 0.9, 'Early spring'),
                    (4, 1.0, 'Spring'), (5, 1.3, 'Summer start'), (6, 1.5, 'Summer peak'),
                    (7, 1.6, 'Summer high'), (8, 1.5, 'Summer'), (9, 1.2, 'Early fall'),
                    (10, 1.0, 'Fall'), (11, 0.9, 'Late fall'), (12, 1.1, 'Holiday season')
                ],
                'Frozen': [
                    (1, 1.1, 'Winter comfort'), (2, 1.0, 'Winter'), (3, 0.9, 'Early spring'),
                    (4, 0.8, 'Spring'), (5, 0.7, 'Late spring'), (6, 0.6, 'Summer low'),
                    (7, 0.6, 'Summer low'), (8, 0.7, 'Summer'), (9, 0.9, 'Early fall'),
                    (10, 1.0, 'Fall'), (11, 1.1, 'Late fall'), (12, 1.3, 'Holiday season')
                ],
                'Snacks': [
                    (1, 1.2, 'Winter comfort'), (2, 1.1, 'Winter'), (3, 1.0, 'Early spring'),
                    (4, 0.9, 'Spring'), (5, 0.8, 'Late spring'), (6, 0.7, 'Summer low'),
                    (7, 0.8, 'Summer'), (8, 0.9, 'Summer end'), (9, 1.0, 'Early fall'),
                    (10, 1.1, 'Fall'), (11, 1.3, 'Holiday prep'), (12, 1.5, 'Holiday peak')
                ],
                'Bakery': [
                    (1, 1.1, 'Winter comfort'), (2, 1.0, 'Winter'), (3, 0.9, 'Early spring'),
                    (4, 0.95, 'Spring'), (5, 1.0, 'Late spring'), (6, 1.1, 'Summer'),
                    (7, 1.0, 'Summer'), (8, 1.05, 'Summer end'), (9, 1.1, 'Early fall'),
                    (10, 1.2, 'Fall'), (11, 1.3, 'Holiday prep'), (12, 1.4, 'Holiday peak')
                ],
                'Meat': [
                    (1, 1.0, 'Winter'), (2, 0.95, 'Winter'), (3, 0.9, 'Early spring'),
                    (4, 0.95, 'Spring'), (5, 1.1, 'Barbecue start'), (6, 1.3, 'Summer peak'),
                    (7, 1.4, 'Summer high'), (8, 1.3, 'Summer'), (9, 1.1, 'Early fall'),
                    (10, 1.0, 'Fall'), (11, 1.2, 'Holiday prep'), (12, 1.5, 'Holiday peak')
                ]
            }
            conn = get_db_connection()
            cur = conn.cursor()
            for category, patterns in seasonal_data.items():
                for month, factor, description in patterns:
                    cur.execute("""
                        INSERT OR REPLACE INTO SeasonalPatterns
                        (Category, Month, SeasonalityFactor, Description)
                        VALUES (?, ?, ?, ?)
                    """, (category, month, factor, description))
            # Add default patterns for other categories
            default_patterns = [(m, 1.0, 'Standard') for m in range(1, 13)]
            categories = ['Toiletries', 'Cleaning', 'Alcohol', 'Pet', 'Baby', 'Unknown']
            for category in categories:
                for month, factor, description in default_patterns:
                    cur.execute("""
                        INSERT OR IGNORE INTO SeasonalPatterns
                        (Category, Month, SeasonalityFactor, Description)
                        VALUES (?, ?, ?, ?)
                    """, (category, month, factor, description))
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"Error initializing seasonal patterns: {e}")

    def initialize_holidays(self):
        """Initialize holiday data for the current year"""
        try:
            us_holidays = holidays.US(years=datetime.now().year)
            conn = get_db_connection()
            cur = conn.cursor()
            start_date = datetime(datetime.now().year, 1, 1)
            end_date = datetime(datetime.now().year, 12, 31)
            current_date = start_date
            while current_date <= end_date:
                is_holiday = current_date in us_holidays
                holiday_name = us_holidays.get(current_date) if is_holiday else None
                if is_holiday:
                    holiday_lower = holiday_name.lower() if holiday_name else ""
                    if any(x in holiday_lower for x in ['christmas', 'thanksgiving']):
                        weather_impact = 1.3
                    elif any(x in holiday_lower for x in ['easter', 'labor']):
                        weather_impact = 1.15
                    else:
                        weather_impact = 1.05
                else:
                    weather_impact = 1.0
                cur.execute("""
                    INSERT OR REPLACE INTO ExternalFactors
                    (FactorDate, IsHoliday, HolidayName, WeatherImpact, EconomicIndex)
                    VALUES (?, ?, ?, ?, ?)
                """, (current_date.date(), is_holiday, holiday_name, weather_impact, 1.0))
                current_date += timedelta(days=1)
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"Warning: {e}")

    def get_seasonal_factor(self, category, date):
        """Get seasonal factor for a category and date"""
        try:
            month = date.month
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("""
                SELECT SeasonalityFactor FROM SeasonalPatterns
                WHERE Category = ? AND Month = ?
            """, (category, month))
            result = cur.fetchone()
            conn.close()
            return result[0] if result else 1.0
        except:
            return 1.0

    def get_holiday_impact(self, date):
        """Get holiday impact for a date"""
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("""
                SELECT WeatherImpact FROM ExternalFactors
                WHERE FactorDate = ?
            """, (date,))
            result = cur.fetchone()
            conn.close()
            return result[0] if result else 1.0
        except:
            return 1.0

    def get_active_promotions(self, product_id, forecast_start, forecast_end):
        """Get active promotions for a product during forecast period"""
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("""
                SELECT PromotionName, DiscountPercentage, StartDate, EndDate, ExpectedLiftPercentage
                FROM Promotions
                WHERE ProductID = ? AND Status = 'active'
                AND ((StartDate BETWEEN ? AND ?) OR (EndDate BETWEEN ? AND ?)
                     OR (StartDate <= ? AND EndDate >= ?))
            """, (product_id, forecast_start, forecast_end, forecast_start, forecast_end,
                  forecast_start, forecast_end))
            promotions = cur.fetchall()
            conn.close()
            return [{
                'name': p[0],
                'discount': p[1],
                'start': p[2],
                'end': p[3],
                'expected_lift': p[4]
            } for p in promotions]
        except:
            return []

    def calculate_promotion_impact(self, promotions, forecast_days):
        """Calculate the impact of promotions on demand"""
        if not promotions:
            return 1.0
        total_impact = 0
        total_days = 0
        for promo in promotions:
            try:
                promo_start = datetime.strptime(promo['start'], '%Y-%m-%d').date()
                promo_end = datetime.strptime(promo['end'], '%Y-%m-%d').date()
                forecast_end = datetime.now().date() + timedelta(days=forecast_days)
                overlap_start = max(datetime.now().date(), promo_start)
                overlap_end = min(forecast_end, promo_end)
                overlap_days = max(0, (overlap_end - overlap_start).days + 1)
                if overlap_days > 0:
                    discount_effect = promo['discount'] * 0.003
                    total_impact_days = promo['expected_lift'] + discount_effect
                    total_impact += overlap_days * total_impact_days
                    total_days += overlap_days
            except Exception as e:
                continue
        if total_days == 0:
            return 1.0
        avg_impact = total_impact / total_days
        promotion_days_ratio = total_days / forecast_days
        overall_impact = 1.0 + (avg_impact * promotion_days_ratio)
        return min(overall_impact, 2.0)

    def multi_model_forecast(self, product_id, days_to_forecast=7, historical_weeks=12):
        """Enhanced forecasting using multiple models and factors"""
        try:
            conn = get_db_connection()
            end_date = datetime.now().date()
            start_date = end_date - timedelta(weeks=historical_weeks)
            query = """
            SELECT sh.SaleDate, sh.QuantitySold, p.Category
            FROM SalesHistory sh
            JOIN Products p ON sh.ProductID = p.ProductID
            WHERE sh.ProductID = ?
            AND sh.SaleDate >= ?
            ORDER BY sh.SaleDate
            """
            df = pd.read_sql_query(query, conn, params=(product_id, start_date))
            conn.close()
            if df.empty or len(df) < 7:
                baseline = self.get_category_baseline(product_id)
                return baseline * days_to_forecast, 0.3
            df['SaleDate'] = pd.to_datetime(df['SaleDate'])
            df = df.set_index('SaleDate')
            daily_sales = df['QuantitySold'].resample('D').sum().reindex(
                pd.date_range(start=start_date, end=end_date), fill_value=0
            )
            forecasts = []
            weights = []
            if len(daily_sales) >= 7:
                ma_7 = daily_sales.tail(7).mean()
                forecasts.append(ma_7)
                weights.append(0.4)
            if len(daily_sales) >= 14:
                recent_weights = np.array(range(1, 8))
                wma = np.average(daily_sales.tail(7).values, weights=recent_weights)
                forecasts.append(wma)
                weights.append(0.3)
            if len(daily_sales) >= 14:
                try:
                    X = np.array(range(len(daily_sales))).reshape(-1, 1)
                    y = daily_sales.values
                    lr = LinearRegression()
                    lr.fit(X, y)
                    future_day = len(daily_sales) + (days_to_forecast // 2)
                    lr_forecast = lr.predict([[future_day]])[0]
                    forecasts.append(max(0, lr_forecast))
                    weights.append(0.3)
                except:
                    pass
            if forecasts:
                total_weight = sum(weights)
                normalized_weights = [w/total_weight for w in weights]
                daily_forecast = np.average(forecasts, weights=normalized_weights)
                data_quality = min(1.0, len(daily_sales) / 90)
                method_agreement = 1.0 - (np.std(forecasts) / (np.mean(forecasts) + 1e-6))
                confidence = 0.3 + (data_quality * 0.4) + (method_agreement * 0.3)
                confidence = min(0.95, max(0.3, confidence))
            else:
                daily_forecast = self.get_category_baseline(product_id)
                confidence = 0.3
            return daily_forecast * days_to_forecast, confidence
        except Exception as e:
            print(f"Forecasting error: {e}")
            baseline = self.get_category_baseline(product_id)
            return baseline * days_to_forecast, 0.3

    def get_category_baseline(self, product_id):
        """Get baseline demand based on product category"""
        try:
            conn = get_db_connection()
            query = "SELECT Category FROM Products WHERE ProductID = ?"
            df = pd.read_sql_query(query, conn, params=(product_id,))
            conn.close()
            if not df.empty:
                category = df.iloc[0]['Category']
                baselines = {
                    'Dairy': 8.0, 'Beverages': 12.0, 'Frozen': 6.0, 'Meat': 5.0,
                    'Toiletries': 4.0, 'Cleaning': 3.0, 'Alcohol': 7.0, 'Snacks': 15.0,
                    'Bakery': 10.0, 'Pet': 3.0, 'Baby': 2.0
                }
                return baselines.get(category, 6.0)
            return 6.0
        except:
            return 6.0

    def enhanced_forecasting(self, product_id, days_to_forecast=7, historical_weeks=12, include_promotions=True):
        """Complete enhanced forecasting with all factors"""
        try:
            base_demand, base_confidence = self.multi_model_forecast(product_id, days_to_forecast, historical_weeks)
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("SELECT Category FROM Products WHERE ProductID = ?", (product_id,))
            result = cur.fetchone()
            conn.close()
            category = result[0] if result else "Unknown"
            seasonal_factors = []
            for i in range(days_to_forecast):
                forecast_date = datetime.now().date() + timedelta(days=i)
                seasonal_factor = self.get_seasonal_factor(category, forecast_date)
                holiday_factor = self.get_holiday_impact(forecast_date)
                combined_factor = seasonal_factor * holiday_factor
                seasonal_factors.append(combined_factor)
            avg_seasonal_factor = np.mean(seasonal_factors) if seasonal_factors else 1.0
            seasonal_demand = base_demand * avg_seasonal_factor
            promotion_impact = 1.0
            promotions = []
            if include_promotions:
                forecast_start = datetime.now().date()
                forecast_end = forecast_start + timedelta(days=days_to_forecast)
                promotions = self.get_active_promotions(product_id, forecast_start, forecast_end)
                promotion_impact = self.calculate_promotion_impact(promotions, days_to_forecast)
                final_demand = seasonal_demand * promotion_impact
            else:
                final_demand = seasonal_demand
            confidence_factors = []
            confidence_factors.append(base_confidence)
            confidence_factors.append(0.7)
            confidence_factors.append(0.6 if promotions else 0.8)
            final_confidence = np.mean(confidence_factors)
            factors = {
                "base_demand": round(base_demand, 2),
                "seasonal_factor": round(avg_seasonal_factor, 3),
                "promotion_impact": round(promotion_impact, 3),
                "promotions_active": len(promotions),
                "confidence_factors": [round(cf, 3) for cf in confidence_factors],
                "forecast_methods_used": len([w for w in confidence_factors if w > 0])
            }
            return final_demand, final_confidence, factors
        except Exception as e:
            print(f"Enhanced forecasting error: {e}")
            baseline = self.get_category_baseline(product_id) * days_to_forecast
            return baseline, 0.3, {"error": str(e), "fallback_used": True}

# Initialize enhanced forecasting
forecasting_engine = EnhancedForecastingEngine()

# ========== PREDICTION ACCURACY TRACKING ==========

class PredictionAccuracy:
    def __init__(self):
        pass

    def log_prediction(self, product_id, predicted_demand, period_days, period_end_date):
        """Log prediction for future accuracy calculation"""
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("""
                INSERT INTO PredictionAccuracy
                (ProductID, PredictedDemand, PeriodDays, PredictionDate, PeriodEndDate)
                VALUES (?, ?, ?, date('now'), ?)
            """, (product_id, predicted_demand, period_days, period_end_date))
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"Error logging prediction: {e}")

    def calculate_accuracy(self, product_id, weeks=8):
        """Calculate accuracy of past predictions"""
        try:
            conn = get_db_connection()
            query = """
            SELECT
                pa.AccuracyID,
                pa.ProductID,
                p.ProductName,
                pa.PredictedDemand,
                pa.PeriodDays,
                pa.PredictionDate,
                pa.PeriodEndDate,
                COALESCE(SUM(sh.QuantitySold), 0) as ActualDemand
            FROM PredictionAccuracy pa
            JOIN Products p ON pa.ProductID = p.ProductID
            LEFT JOIN SalesHistory sh ON pa.ProductID = sh.ProductID
                AND sh.SaleDate BETWEEN pa.PredictionDate AND pa.PeriodEndDate
            WHERE pa.ProductID = ?
                AND pa.PeriodEndDate <= date('now')
                AND pa.PredictionDate >= date('now', '-' || ? || ' weeks')
            GROUP BY pa.AccuracyID
            HAVING ActualDemand > 0
            """
            df = pd.read_sql_query(query, conn, params=(product_id, weeks))
            conn.close()
            if df.empty:
                return {"error": "No completed predictions to evaluate"}
            df['AbsoluteError'] = abs(df['PredictedDemand'] - df['ActualDemand'])
            df['PercentageError'] = (df['AbsoluteError'] / df['ActualDemand']) * 100
            df['Accuracy'] = 100 - df['PercentageError']
            return {
                "product_id": product_id,
                "total_predictions_evaluated": len(df),
                "average_accuracy": round(df['Accuracy'].mean(), 2),
                "average_error": round(df['AbsoluteError']. mean(), 2),
                "best_prediction": {
                    "accuracy": round(df['Accuracy'].max(), 2),
                    "predicted": round(df.loc[df['Accuracy'].idxmax(), 'PredictedDemand'], 2),
                    "actual": int(df.loc[df['Accuracy'].idxmax(), 'ActualDemand'])
                },
                "worst_prediction": {
                    "accuracy": round(df['Accuracy'].min(), 2),
                    "predicted": round(df.loc[df['Accuracy'].idxmin(), 'PredictedDemand'], 2),
                    "actual": int(df.loc[df['Accuracy'].idxmin(), 'ActualDemand'])
                }
            }
        except Exception as e:
            return {"error": str(e)}

# Initialize accuracy tracker
accuracy_tracker = PredictionAccuracy()

# ========== CORE AI FUNCTIONS ==========

def get_stock(product_id):
    """Get current stock levels from store and warehouse"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("SELECT SUM(Quantity) FROM StoreStock WHERE ProductID = ?", (product_id,))
        shelf = cur.fetchone()[0] or 0
        cur.execute("SELECT SUM(Quantity) FROM WarehouseStock WHERE ProductID = ?", (product_id,))
        wh = cur.fetchone()[0] or 0
        conn.close()
        return int(shelf), int(wh)
    except:
        return 0, 0

def advanced_forecasting(product_id, days_to_forecast=7, historical_weeks=8):
    """Advanced forecasting using historical sales data"""
    try:
        conn = get_db_connection()
        end_date = datetime.now().date()
        start_date = end_date - timedelta(weeks=historical_weeks)
        query = """
        SELECT SaleDate, QuantitySold
        FROM SalesHistory
        WHERE ProductID = ?
        AND SaleDate >= ? AND SaleDate <= ?
        ORDER BY SaleDate
        """
        df = pd.read_sql_query(query, conn, params=(product_id, start_date, end_date))
        conn.close()
        if df.empty or len(df) < 7:
            return get_category_baseline(product_id) * days_to_forecast
        df['SaleDate'] = pd.to_datetime(df['SaleDate'])
        df = df.set_index('SaleDate')
        daily_sales = df['QuantitySold'].resample('D').sum().reindex(
            pd.date_range(start=start_date, end=end_date), fill_value=0
        )
        forecasts = []
        if len(daily_sales) >= 14:
            two_week_avg = daily_sales.tail(14).mean()
            forecasts.append(two_week_avg)
        overall_avg = daily_sales.mean()
        forecasts.append(overall_avg)
        if len(daily_sales) >= 7:
            recent_avg = daily_sales.tail(7).mean()
            forecasts.append(recent_avg)
        if len(forecasts) >= 2:
            weights = [0.5, 0.3, 0.2][:len(forecasts)]
            daily_forecast = np.average(forecasts, weights=weights)
        else:
            daily_forecast = np.mean(forecasts)
        return daily_forecast * days_to_forecast
    except Exception as e:
        print(f"Forecasting error: {e}")
        return get_category_baseline(product_id) * days_to_forecast

def get_category_baseline(product_id):
    """Get baseline demand based on product category"""
    try:
        conn = get_db_connection()
        query = "SELECT Category FROM Products WHERE ProductID = ?"
        df = pd.read_sql_query(query, conn, params=(product_id,))
        conn.close()
        if not df.empty:
            category = df.iloc[0]['Category']
            baselines = {
                'Dairy': 8.0, 'Beverages': 12.0, 'Frozen': 6.0, 'Meat': 5.0,
                'Toiletries': 4.0, 'Cleaning': 3.0, 'Alcohol': 7.0, 'Snacks': 15.0,
                'Bakery': 10.0, 'Pet': 3.0, 'Baby': 2.0
            }
            return baselines.get(category, 6.0)
        return 6.0
    except:
        return 6.0

def get_sales_trends(product_id):
    """Analyze sales trends for confidence calculation"""
    try:
        conn = get_db_connection()
        query = """
        SELECT
            SUM(CASE WHEN SaleDate >= date('now', '-7 days') THEN QuantitySold ELSE 0 END) as last_7_days,
            SUM(CASE WHEN SaleDate >= date('now', '-14 days') AND SaleDate < date('now', '-7 days') THEN QuantitySold ELSE 0 END) as previous_7_days,
            COUNT(DISTINCT SaleDate) as active_days
        FROM SalesHistory
        WHERE ProductID = ?
        AND SaleDate >= date('now', '-60 days')
        """
        df = pd.read_sql_query(query, conn, params=(product_id,))
        conn.close()
        if not df.empty:
            trend = "stable"
            last_week = df.iloc[0]['last_7_days'] or 0
            prev_week = df.iloc[0]['previous_7_days'] or 0
            if prev_week > 0:
                change = ((last_week - prev_week) / prev_week) * 100
                if change > 15:
                    trend = "increasing"
                elif change < -15:
                    trend = "decreasing"
            else:
                change = 0
            return {
                "trend": trend,
                "weekly_change_percent": round(change, 2),
                "data_quality": "good" if (df.iloc[0]['active_days'] or 0) > 20 else "limited"
            }
        return {"trend": "unknown", "data_quality": "poor", "weekly_change_percent": 0}
    except:
        return {"trend": "unknown", "data_quality": "poor", "weekly_change_percent": 0}

def calculate_confidence(accuracy_data, sales_trend):
    """Calculate confidence score for predictions"""
    if "average_accuracy" in accuracy_data and accuracy_data["average_accuracy"] > 0:
        accuracy_score = accuracy_data["average_accuracy"] / 100
    else:
        accuracy_score = 0.5
    trend_score = 1.0 if sales_trend["data_quality"] == "good" else 0.6
    stability_score = 0.8 if sales_trend["trend"] == "stable" else 0.5
    overall_confidence = (accuracy_score * 0.5) + (trend_score * 0.3) + (stability_score * 0.2)
    return {
        "score": round(overall_confidence * 100, 2),
        "level": "high" if overall_confidence > 0.8 else "medium" if overall_confidence > 0.6 else "low",
        "factors": {
            "historical_accuracy": round(accuracy_score * 100, 2),
            "data_quality": sales_trend["data_quality"],
            "sales_stability": sales_trend["trend"]
        }
    }

def check_critical_alerts(product_id, recommendation):
    """Check if critical alerts should be sent - EXACT FORMAT AS SPECIFIED"""
    shelf_stock = recommendation["current_shelf"]
    predicted_demand = recommendation["predicted_with_buffer"]

    if shelf_stock <= 5:
        product_name = recommendation.get("product_name", f"Product {product_id}")
        category = recommendation.get("category", "Unknown")
        warehouse_stock = recommendation.get("current_warehouse", 0)

        shortage = max(0, predicted_demand - shelf_stock)
        transfer_units = min(shortage, warehouse_stock)
        order_units = max(0, shortage - transfer_units)

        critical_alert_system.send_critical_stock_alert(
            product_id,
            product_name,
            category,
            shelf_stock,
            warehouse_stock,
            predicted_demand,
            transfer_units,
            order_units
        )

# ========== CRITICAL ALERT ENDPOINTS ==========

@app.get("/critical-alerts/history")
def get_critical_alerts_history(days: int = 7):
    """Get history of critical alerts"""
    try:
        alerts = critical_alert_system.get_critical_alert_history(days)
        return {
            "status": "success",
            "total_alerts": len(alerts),
            "time_period": f"{days} days",
            "alerts": alerts
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/test-critical-alert")
def test_critical_alert():
    """Test the critical alert system"""
    try:
        success = critical_alert_system.send_critical_stock_alert(
            1,
            "Milk",
            "Dairy",
            5,
            373,
            0,
            0,
            0
        )
        return {
            "status": "success" if success else "error",
            "message": "Critical alert test completed" if success else "Test failed",
            "test_alert_sent": success
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

# ========== ENHANCED API ENDPOINTS ==========

@app.post("/enhanced-recommend")
def enhanced_recommend(req: EnhancedRecommendRequest):
    """Enhanced AI recommendation with promotions and seasonality"""
    try:
        final_demand, confidence, factors = forecasting_engine.enhanced_forecasting(
            req.product_id, req.period_days, req.historical_weeks, req.include_promotions
        )
        predicted_with_buffer = final_demand * (1.0 + req.safety_buffer)
        shelf, wh = get_stock(req.product_id)
        shortage = max(0, predicted_with_buffer - shelf)
        transfer = min(shortage, wh)
        order = max(0, shortage - transfer)
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("SELECT ProductName, Category FROM Products WHERE ProductID = ?", (req.product_id,))
        product_info = cur.fetchone()
        conn.close()
        product_name = product_info[0] if product_info else "Unknown"
        category = product_info[1] if product_info else "Unknown"
        forecast_start = datetime.now().date()
        forecast_end = forecast_start + timedelta(days=req.period_days)
        promotions = forecasting_engine.get_active_promotions(req.product_id, forecast_start, forecast_end)

        recommendation = {
            "product_id": req.product_id,
            "product_name": product_name,
            "category": category,
            "predicted_demand": round(final_demand, 2),
            "predicted_with_buffer": round(predicted_with_buffer, 2),
            "period_days": req.period_days,
            "ideal_stock": round(predicted_with_buffer, 2),
            "current_shelf": shelf,
            "current_warehouse": wh,
            "recommended_transfer": int(transfer),
            "recommended_order": int(order),
            "safety_buffer": req.safety_buffer,
            "confidence_score": round(confidence, 3),
            "forecasting_factors": factors,
            "active_promotions": promotions,
            "forecast_type": "enhanced",
            "status": "success"
        }

        check_critical_alerts(req.product_id, recommendation)
        return recommendation

    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/create-promotion")
def create_promotion(req: PromotionRequest):
    """Create a new promotion"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO Promotions
            (ProductID, PromotionName, DiscountPercentage, StartDate, EndDate, ExpectedLiftPercentage)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (req.product_id, req.promotion_name, req.discount_percentage,
              req.start_date, req.end_date, req.expected_lift))
        conn.commit()
        conn.close()
        return {
            "status": "success",
            "message": f"Promotion '{req.promotion_name}' created for product {req.product_id}"
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/product-promotions/{product_id}")
def get_product_promotions(product_id: int):
    """Get all promotions for a product"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            SELECT PromotionID, PromotionName, DiscountPercentage, StartDate, EndDate,
                   ExpectedLiftPercentage, Status, CreatedDate
            FROM Promotions
            WHERE ProductID = ?
            ORDER BY StartDate DESC
        """, (product_id,))
        promotions = cur.fetchall()
        conn.close()
        return {
            "product_id": product_id,
            "promotions": [{
                "id": p[0],
                "name": p[1],
                "discount": p[2],
                "start_date": p[3],
                "end_date": p[4],
                "expected_lift": p[5],
                "status": p[6],
                "created_date": p[7]
            } for p in promotions]
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/demand-forecast/{product_id}")
def get_demand_forecast(product_id: int, days: int = 30):
    """Get detailed demand forecast for a product"""
    try:
        forecasts = []
        total_demand, total_confidence, total_factors = forecasting_engine.enhanced_forecasting(
            product_id, days, 12, True
        )
        for day in range(1, min(days, 31) + 1):
            daily_demand, daily_confidence, daily_factors = forecasting_engine.enhanced_forecasting(
                product_id, day, 12, True
            )
            forecast_date = datetime.now().date() + timedelta(days=day-1)
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("SELECT Category FROM Products WHERE ProductID = ?", (product_id,))
            result = cur.fetchone()
            conn.close()
            category = result[0] if result else "Unknown"
            seasonal_factor = forecasting_engine.get_seasonal_factor(category, forecast_date)
            holiday_factor = forecasting_engine.get_holiday_impact(forecast_date)
            forecasts.append({
                "date": forecast_date.isoformat(),
                "day": day,
                "predicted_demand": round(daily_demand / max(day, 1), 2),
                "cumulative_demand": round(daily_demand, 2),
                "confidence": round(daily_confidence, 3),
                "seasonal_factor": round(seasonal_factor, 3),
                "holiday_factor": round(holiday_factor, 3)
            })
        return {
            "product_id": product_id,
            "forecast_period_days": days,
            "total_predicted_demand": round(total_demand, 2),
            "average_daily_demand": round(total_demand / days, 2),
            "overall_confidence": round(total_confidence, 3),
            "forecasting_factors": total_factors,
            "daily_forecasts": forecasts,
            "status": "success"
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

# ========== ORIGINAL API ENDPOINTS ==========

@app.post("/recommend")
def recommend(req: RecommendRequest):
    """AI recommendation using historical sales data"""
    try:
        period_demand = advanced_forecasting(
            req.product_id,
            req.period_days,
            req.historical_weeks
        )
        predicted_with_buffer = period_demand * (1.0 + req.safety_buffer)
        shelf, wh = get_stock(req.product_id)
        shortage = max(0, predicted_with_buffer - shelf)
        transfer = min(shortage, wh)
        order = max(0, shortage - transfer)
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("SELECT ProductName, Category FROM Products WHERE ProductID = ?", (req.product_id,))
        product_info = cur.fetchone()
        conn.close()
        product_name = product_info[0] if product_info else "Unknown Product"
        category = product_info[1] if product_info else "Unknown"
        accuracy_data = accuracy_tracker.calculate_accuracy(req.product_id, 8)
        sales_trend = get_sales_trends(req.product_id)
        confidence = calculate_confidence(accuracy_data, sales_trend)
        period_end_date = datetime.now().date() + timedelta(days=req.period_days)
        accuracy_tracker.log_prediction(
            req.product_id,
            period_demand,
            req.period_days,
            period_end_date
        )

        recommendation = {
            "product_id": req.product_id,
            "product_name": product_name,
            "category": category,
            "predicted_daily": round(period_demand / req.period_days, 2),
            "predicted_daily_with_buffer": round(predicted_with_buffer / req.period_days, 2),
            "predicted_period_demand": round(period_demand, 2),
            "predicted_with_buffer": round(predicted_with_buffer, 2),
            "period_days": req.period_days,
            "ideal_stock": round(predicted_with_buffer, 2),
            "current_shelf": shelf,
            "current_warehouse": wh,
            "recommended_transfer": int(transfer),
            "recommended_order": int(order),
            "safety_buffer": req.safety_buffer,
            "confidence_level": confidence,
            "historical_accuracy": accuracy_data,
            "sales_trend": sales_trend,
            "status": "success"
        }

        check_critical_alerts(req.product_id, recommendation)
        return recommendation

    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/log-sale")
def log_sale_endpoint(req: SalesLogRequest):
    """Endpoint to log sales transactions"""
    try:
        sale_date = req.sale_date or datetime.now().date()
        if isinstance(sale_date, str):
            sale_date = datetime.strptime(sale_date, '%Y-%m-%d').date()
        day_of_week = sale_date.weekday()
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO SalesHistory (ProductID, SaleDate, QuantitySold, DayOfWeek, WasPromotion)
            VALUES (?, ?, ?, ?, ?)
        """, (req.product_id, sale_date, req.quantity_sold, day_of_week, req.was_promotion))
        conn.commit()
        conn.close()
        return {
            "status": "success",
            "message": f"Sale logged for product {req.product_id}",
            "quantity": req.quantity_sold
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/generate-sample-data")
def generate_sample_data():
    """Generate sample sales data for testing"""
    try:
        generate_enhanced_sample_data()
        return {"status": "success", "message": "Enhanced sample data generated"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/generate-enhanced-sample-data")
def generate_enhanced_sample_data_endpoint():
    """Generate enhanced sample data with promotions"""
    try:
        generate_enhanced_sample_data()
        return {"status": "success", "message": "Enhanced sample data with promotions generated"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/sales-history/{product_id}")
def get_sales_history(product_id: int, weeks: int = 8):
    """Get sales history for a product"""
    try:
        conn = get_db_connection()
        end_date = datetime.now().date()
        start_date = end_date - timedelta(weeks=weeks)
        query = """
        SELECT SaleDate, QuantitySold, DayOfWeek, WasPromotion
        FROM SalesHistory
        WHERE ProductID = ?
        AND SaleDate >= ?
        ORDER BY SaleDate
        """
        df = pd.read_sql_query(query, conn, params=(product_id, start_date))
        conn.close()
        return {
            "product_id": product_id,
            "analysis_period": f"{weeks} weeks",
            "total_sales": int(df['QuantitySold'].sum()) if not df.empty else 0,
            "promotional_sales": int(df[df['WasPromotion'] == 1]['QuantitySold'].sum()) if not df.empty else 0,
            "sales_records": df.to_dict('records') if not df.empty else []
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/prediction-accuracy/{product_id}")
def get_prediction_accuracy(product_id: int, weeks: int = 8):
    """Get accuracy metrics for past predictions"""
    return accuracy_tracker.calculate_accuracy(product_id, weeks)

@app.get("/prediction-confidence/{product_id}")
def get_prediction_confidence(product_id: int):
    """Get confidence analysis for a product"""
    accuracy_data = accuracy_tracker.calculate_accuracy(product_id, 8)
    sales_trend = get_sales_trends(product_id)
    confidence = calculate_confidence(accuracy_data, sales_trend)
    return {
        "product_id": product_id,
        "confidence_level": confidence,
        "historical_accuracy": accuracy_data,
        "sales_trend": sales_trend
    }

@app.get("/seasonal-patterns/{category}")
def get_seasonal_patterns(category: str):
    """Get seasonal patterns for a category"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            SELECT Month, SeasonalityFactor, Description
            FROM SeasonalPatterns
            WHERE Category = ?
            ORDER BY Month
        """, (category,))
        patterns = cur.fetchall()
        conn.close()
        return {
            "category": category,
            "patterns": [{
                "month": p[0],
                "factor": p[1],
                "description": p[2]
            } for p in patterns]
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/test")
def test():
    return {"message": "Enhanced AI Forecasting Server is working!", "status": "success"}

@app.get("/server-url")
def get_server_url():
    """Return the current server URL for Unity client"""
    return {
        "url": PUBLIC_URL,
        "status": "success",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/")
def root():
    return {
        "message": "Enhanced Supermarket AI Forecasting Server",
        "status": "running",
        "url": PUBLIC_URL,
        "endpoints": [
            "/test - Server health check",
            "/server-url - Get current server URL",
            "/recommend - Basic AI recommendations",
            "/enhanced-recommend - Enhanced AI with promotions & seasonality",
            "/create-promotion - Create new promotion",
            "/product-promotions/{id} - Get product promotions",
            "/demand-forecast/{id} - Detailed demand forecast",
            "/log-sale - Record sales",
            "/sales-history/{id} - Sales data",
            "/prediction-accuracy/{id} - Accuracy metrics",
            "/prediction-confidence/{id} - Get confidence analysis",
            "/seasonal-patterns/{category} - Seasonal patterns",
            "/generate-sample-data - Generate sample data",
            "/generate-enhanced-sample-data - Generate enhanced sample data",
            "/critical-alerts/history - View critical alert history",
            "/test-critical-alert - Test critical alert system",
            "/generate-realistic-products - Generate realistic products with brands",
            "/generate-one-year-data - Generate one year of sales data",
            "/generate-realistic-promotions - Generate realistic promotions",
            "/initialize-realistic-stock - Initialize realistic stock levels",
            "/generate-complete-dataset - Generate complete realistic dataset",
            "/dataset-stats - Get dataset statistics",
            "/visualization/linear-regression/{id} - Linear regression diagram",
            "/visualization/seasonal-sales/{id} - Seasonal sales diagram",
            "/visualization/confidence-assessment - Confidence assessment diagram",
            "/visualization/promotion-effect/{id} - Promotion effect diagram",
            "/visualization/all-diagrams/{id} - All visualization diagrams"
        ]
    }

# ========== SERVER STARTUP ==========

def test_server(server_url):
    """Test the server after it starts"""
    time.sleep(3)
    print("Testing AI server...")
    try:
        response = requests.get(f"{server_url}/test", verify=False)
        print("Server test:", response.json())
        response = requests.post(f"{server_url}/recommend",
                               json={"product_id": 1, "historical_weeks": 4}, verify=False)
        print("Basic AI test:", response.json())
        response = requests.post(f"{server_url}/enhanced-recommend",
                               json={"product_id": 1, "historical_weeks": 8, "include_promotions": True}, verify=False)
        print("Enhanced AI test:", response.json())
    except Exception as e:
        print(f"Test failed: {e}")

def generate_sample_visualizations():
    """Generate sample visualizations for demonstration"""
    print("\n" + "="*70)
    print("GENERATING SAMPLE VISUALIZATIONS")
    print("="*70)

    # Generate sample data first
    generate_enhanced_sample_data()

    # Create visualizations for a sample product
    sample_product_id = 1

    print("1. Creating Linear Regression Diagram...")
    lr_image = create_linear_regression_diagram(sample_product_id, display_plot=True)
    if lr_image:
        print("✓ Linear Regression Diagram created successfully")

    print("2. Creating Seasonal Sales Diagram...")
    ss_image = create_seasonal_sales_diagram(sample_product_id, display_plot=True)
    if ss_image:
        print("✓ Seasonal Sales Diagram created successfully")

    print("3. Creating Confidence Assessment Diagram...")
    ca_image = create_confidence_assessment_diagram(display_plot=True)
    if ca_image:
        print("✓ Confidence Assessment Diagram created successfully")

    print("4. Creating Promotion Effect Diagram...")
    pe_image = create_promotion_effect_diagram(sample_product_id, display_plot=True)
    if pe_image:
        print("✓ Promotion Effect Diagram created successfully")

    print("✓ All sample visualizations generated successfully!")
    print("="*70)

print("Starting ENHANCED AI Forecasting Server with REALISTIC DATA & VISUALIZATIONS...")

# Generate sample visualizations on startup
generate_sample_visualizations()

# Kill any existing ngrok tunnels
try:
    ngrok.kill()
    print("Killed existing ngrok tunnels")
except:
    print("No existing ngrok tunnels to kill")

# ========== NGROK CONFIGURATION ==========
NGROK_AUTH_TOKEN = "33YB5MQL3fJnJSDsnBWKqbxHR9y_7MUinPVnKZBQb5oDC9UsD"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Get a fresh URL using your domain
print("Getting new ngrok URL...")
tunnel = ngrok.connect(8000)
PUBLIC_URL = tunnel.public_url
print(f"New URL: {PUBLIC_URL}")

# Start server in a thread
def run_server():
    print("Starting FastAPI server on port 8000...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")

server_thread = threading.Thread(target=run_server)
server_thread.daemon = True
server_thread.start()

# Test both systems
print("Starting server tests...")
time.sleep(5)  # Wait for server to start

# Only test the server, don't send unnecessary test emails
test_server(PUBLIC_URL)

print("\n" + "="*70)
print("ENHANCED AI FORECASTING SERVER WITH VISUALIZATIONS IS RUNNING!")
print("="*70)
print(f"UNITY URL: {PUBLIC_URL}")
print(f"\nIn Unity, set bootstrapUrl to:")
print(f'public string bootstrapUrl = "{PUBLIC_URL}";')

print("\nREALISTIC DATA FEATURES:")
print("✓ 150+ Real Products with actual supermarket brands")
print("✓ One full year of comprehensive sales data")
print("✓ Seasonal patterns and holiday sales boosts")
print("✓ Realistic promotional campaigns throughout the year")
print("✓ Appropriate stock levels for all product categories")

print("\nVISUALIZATION FEATURES:")
print("✓ Linear Regression Diagrams for sales forecasting")
print("✓ Seasonal Sales Patterns for specific products")
print("✓ Confidence Assessment for AI predictions")
print("✓ Promotion Effect Analysis with detailed metrics")

print("\nVISUALIZATION ENDPOINTS:")
print("GET /visualization/linear-regression/{id} - Linear regression analysis")
print("GET /visualization/seasonal-sales/{id} - Monthly sales patterns")
print("GET /visualization/confidence-assessment - AI confidence scores")
print("GET /visualization/promotion-effect/{id} - Promotion effectiveness")
print("GET /visualization/all-diagrams/{id} - All diagrams for a product")

print("\nENHANCED ENDPOINTS:")
print("POST /enhanced-recommend - AI with promotions & seasonality")
print("POST /create-promotion - Create new promotions")
print("GET  /product-promotions/{id} - Get product promotions")
print("GET  /demand-forecast/{id} - 30-day demand forecast")
print("GET  /seasonal-patterns/{category} - Seasonal patterns")

print("\nREALISTIC DATA GENERATION ENDPOINTS:")
print("POST /generate-realistic-products - Create products with real brands")
print("POST /generate-one-year-data - Generate full year sales data")
print("POST /generate-realistic-promotions - Create seasonal promotions")
print("POST /initialize-realistic-stock - Set realistic inventory")
print("POST /generate-complete-dataset - Generate everything at once")
print("GET  /dataset-stats - View dataset statistics")

print("\nCRITICAL ALERT ENDPOINTS:")
print("GET  /critical-alerts/history - View critical alert history")
print("POST /test-critical-alert - Test the critical alert system")

print("\nBASIC ENDPOINTS:")
print("POST /recommend - Basic AI recommendations")
print("GET  /prediction-accuracy/{id} - View prediction accuracy")
print("GET  /prediction-confidence/{id} - Get confidence analysis")
print("POST /log-sale - Record sales transactions")
print("GET  /sales-history/{id} - View sales history")

print(f"\nCRITICAL ALERT CONFIGURATION:")
print(f"Recipients: {', '.join(EMAIL_CONFIG['critical_alert_emails'])}")
print(f"SMTP Server: {EMAIL_CONFIG['smtp_server']}")
print(f"Sender: {EMAIL_CONFIG['sender_email']}")

print(f"\nStarted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# Keep running
try:
    while True:
        time.sleep(10)
        print(f"Server running... {datetime.now().strftime('%H:%M:%S')}")
except KeyboardInterrupt:
    print("Server stopped by user")

